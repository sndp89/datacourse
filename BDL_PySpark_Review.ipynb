{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Review\n",
    "<!-- requirement: small_data/employee -->\n",
    "<!-- requirement: small_data/gutenberg -->\n",
    "<!-- requirement: small_data/eeg -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Spark__ is a low-level system for distributed computation on a cluster.  It improves on older systems, like MapReduce, with two major advantages:\n",
    " \n",
    " - It can do in-memory caching between stages, which improves performance.  It also makes it suitable for classes of algorithms that would otherwise be too slow (*e.g.* iterative algorithms, like the training step in many machine learning algorithms).\n",
    " \n",
    " - It has a more flexible execution model and a more expressive API.\n",
    " \n",
    "Spark is written in Scala, but there are very good Python bindings in **PySpark**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main abstraction in Spark is the **Resilient Distributed Data Set** (RDD).  This is like a (smartly) distributed list, which is partitioned across the nodes of the cluster and can be operated on in parallel.  The operations take the form of the usual functional list operations.  There are two types of operations: Transformations and Actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations and Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Transformations:** These create a new RDD from an old one, for instance `map` or `flatMap`.  Transformations in Spark are _lazy_, which means that they do not compute the answer right away&mdash;computation occurs only when the value is needed by something else.  Instead they represent the \"idea\" of the transformation applied to the base data set (*e.g.* for each chaining).  In particular, this means that intermediate results to computations do not necessarily get created and stored&mdash;if the output of your map is going straight into a reduce, Spark should be clever enough to realize this and not actually store the output of the map.  Another consequence of this design is that the results of transformations are, by default, recomputed each time they are needed&mdash;to store them one must explicitly call `cache`.\n",
    "    \n",
    "    [Transformations](https://spark.apache.org/docs/latest/programming-guide.html#transformations) typically *transform* the data and return another RDD.  Common examples include `map`, `filter`, `flatMap`, and also `join`, `cogroup`, `groupByKey`, `reduceByKey`, `countByKey`, and `sample`.\n",
    "\n",
    "- **Actions:** These actually return a value as a result of a computation.  For instance, `reduce` is an action that combines all elements of an RDD using some function and then returns the final result.  (Note that `reduceByKey` actually returns an RDD, and therefore is a transformation.)\n",
    "    \n",
    "    [Actions](https://spark.apache.org/docs/latest/programming-guide.html#actions) typically either generate *small* outputs (e.g. `reduce`, `count`, `first`, `take`, `takeSample`, `foreach`, `collect`) or persist to disk (e.g. `saveAsTextFile`, `saveAsSequenceFile`, etc.).\n",
    "\n",
    "\n",
    "**Questions:**\n",
    "1. What's the difference between `map` and `foreach` in (non-Spark) Scala?  Why is `map` a transformation but `foreach` an action in Spark?\n",
    "1. Why is `reduceByKey` a transformation but `reduce` an action?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common distributed-computing example is word count: count the number of times a word appears in a large corpus of text.  It is easily parallelizable, since counts can be made on different parts of the text and then added together to get the final count.\n",
    "\n",
    "We start off by making a Spark **context**.  This object manages the resources available to a Spark job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local[*]\", \"temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark will work on a distributed file system by default.  To tell it to deal with local files, we introduce the `localpath` convenience function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "def localpath(path):\n",
    "    return 'file://' + os.path.join(os.path.abspath(os.path.curdir), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `textFile` function, we create a RDD that contains all of the lines for all of the files in a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(localpath('small_data/gutenberg/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `count` action will tell us the total number of lines in these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do useful calculations, we will chain transformations together.  Here, a `flatMap` converts the RDD of lines to a RDD of words.  The `map` makes a tuple of `(word, 1)`.  The word acts as a key for the `reduceByKey` transformation, which sums up the values associated with each key, giving us the total count for each word.  Finally, we sort the RDD and write it out to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/tmp/output_gutenberg\" + str(time.time())\n",
    "\n",
    "lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "     .map(lambda word: (word.lower(), 1)) \\\n",
    "     .reduceByKey(lambda x, y: x + y) \\\n",
    "     .sortByKey() \\\n",
    "     .saveAsTextFile(localpath(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head $filename/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happened in each of these steps.  If we just looks at `lines`, all we see is that it is an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't see any of the lines themselves.  This is a result of Spark's lazy computation.  Only when an action requests some data is a computation actually run.  The `collect()` action will gather all of the elements in the RDD into a Python list.  We don't want all of these, so we'll use `take(5)` to gather five elements from the RDD.  Spark will do the minimal computation necessary to get those five elements; here we see that they come from the beginning of one of the books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elements in the `lines` RDD are whole lines.  The `flatMap` transformation will create a new RDD containing the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "     .take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the difference from the basic `map` transformation, which would convert each line into a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.map(lambda line: line.split(\" \")) \\\n",
    "     .take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use a `map`, now to create those key-value pairs.  We're keying by the word, with the count as the value.  Each time we see a word, it's just a count of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "     .map(lambda word: (word.lower(), 1)) \\\n",
    "     .take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `reduceByKey` will gather like keys together into a single entry, whose value is the result of running the specified reduction function, in this case summation, on all the values associated with that key.  This give us the counts per word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "     .map(lambda word: (word.lower(), 1)) \\\n",
    "     .reduceByKey(lambda x, y: x + y) \\\n",
    "     .take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sortByKey` will alphabetize the RDD, for our convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "     .map(lambda word: (word.lower(), 1)) \\\n",
    "     .reduceByKey(lambda x, y: x + y) \\\n",
    "     .sortByKey() \\\n",
    "     .take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** There are a lot of special symbols in the output.  How would you do word count only on pure characters?  Hint: in Python, you can test for regular expressions this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "matches = re.search(\"^\\\\w+$\", \"abc\")\n",
    "matches.group(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: a simple simulation for $\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "numSamples = 100000\n",
    "\n",
    "def generate_point(_):\n",
    "    x = random.random()\n",
    "    y = random.random()\n",
    "    if x * x + y * y < 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "count = sc.parallelize(range(1, numSamples + 1)) \\\n",
    "    .map(generate_point) \\\n",
    "    .reduce(lambda x, y: x + y)\n",
    "    \n",
    "print(\"Pi is roughly \" + str(4.0 * count / numSamples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the word count example, a common workflow is creating an RDD of key-value pairs.  This can be used to group like keys together for a `reduceByKey` or similar step.  Two RDDs with the same keys can also be joined together, allowing us to accomplish the equivalent of a SQL join.\n",
    "\n",
    "As an example, consider the case where we have two tables, stored here as CSV files.  One contains sales information, connecting users to particular transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat small_data/employee/sales.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want some structure to represent each row of data in an RDD.  We could use a list or tuple, but in this case we'll make a simple class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transaction(object):\n",
    "    def __init__(self, transactionId, productId, userId, amount):\n",
    "        self.transactionId = transactionId\n",
    "        self.productId = productId\n",
    "        self.userId = userId\n",
    "        self.amount = int(amount)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_line(cls, line):\n",
    "        data = line.split(\",\")\n",
    "        if data[0] == \"sales\":\n",
    "            return cls(*data[1:])\n",
    "        return None\n",
    "\n",
    "transactions = sc.textFile(localpath(\"small_data/employee/sales.csv\")) \\\n",
    "    .map(Transaction.from_line) \\\n",
    "    .filter(lambda x: x is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other table contains information about the users themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat small_data/employee/users.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class User(object):\n",
    "    def __init__(self, userId, email, language, country):\n",
    "        self.userId = userId\n",
    "        self.email = email\n",
    "        self.language = language\n",
    "        self.country = country\n",
    "    \n",
    "    @classmethod\n",
    "    def from_line(cls, line):\n",
    "        data = line.split(\",\")\n",
    "        return cls(*data[1:])\n",
    "\n",
    "users = sc.textFile(localpath(\"small_data/employee/users.csv\")) \\\n",
    "    .map(User.from_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the individual RDDs, we can make calculations about the total sales or distribution of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalSales = transactions.map(lambda x: x.amount).sum()\n",
    "usersByCountryCount = users.map(lambda u: (u.country, u)).countByKey()\n",
    "\n",
    "print(\"total sales %f\" % totalSales)\n",
    "print(\"Users by country\")\n",
    "for key, val in usersByCountryCount.items():\n",
    "    print(key, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to know the total sales per country, we need to join these RDDs.  To do this, we need a shared key, which in this case is the user ID.  We use map to transform both RDDs into key-value pairs, and then join to combine them into a single RDD.  This RDD has elements of the form `(userId, (transactionAmount, userCountry))`.  Another map and reduce by key is all we need to get the calculation we desire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salesByCountry = transactions.map(lambda t: (t.userId, t.amount)) \\\n",
    "                .join(users.map(lambda u: (u.userId, u.country))) \\\n",
    "                .map(lambda x: (x[1][1], x[1][0])) \\\n",
    "                .reduceByKey(lambda x, y: x + y) \\\n",
    "                .collect()\n",
    "\n",
    "print(\"Transactions by country\")\n",
    "for country in salesByCountry:\n",
    "    print(country[0], country[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL example\n",
    "\n",
    "The first steps of any data science project usually involve ETL and exploratory analytics on a data set.\n",
    "Here we'll acquaint ourselves both with the Spark shell and some Spark functions that we can use to perform such tasks.\n",
    "\n",
    "#### The data set\n",
    "From UC Irvine's [Machine Learning Archive](http://cml.ics.uci.edu/):\n",
    "> \"The first four lines are header information. Line 1 contains the subject identifier and indicates if the subject was an alcoholic (a) or control (c) subject by the fourth letter. Line 4 identifies the matching conditions: a single object shown (`S1 obj`), object 2 shown in a matching condition (`S2 match`), and object 2 shown in a non matching condition (`S2 nomatch`). Line 5 identifies the start of the data from sensor `FP1`. The four columns of data are: the trial number, sensor position, sample number (0-255), and sensor value (in micro volts).\"\n",
    "\n",
    "There are 16,452 rows in this file including the header. Here's a preview of the first 10 lines and last 10 lines:\n",
    "```\n",
    "# co2a0000364.rd\n",
    "# 120 trials, 64 chans, 416 samples 368 post_stim samples\n",
    "# 3.906000 msecs uV\n",
    "# S1 obj , trial 0\n",
    "# FP1 chan 0\n",
    "0 FP1 0 -8.921\n",
    "0 FP1 1 -8.433\n",
    "0 FP1 2 -2.574\n",
    "0 FP1 3 5.239\n",
    "0 FP1 4 11.587\n",
    "    ...\n",
    "0 Y 246 24.150\n",
    "0 Y 247 20.243\n",
    "0 Y 248 11.454\n",
    "0 Y 249 4.618\n",
    "0 Y 250 3.153\n",
    "0 Y 251 6.571\n",
    "0 Y 252 12.431\n",
    "0 Y 253 15.849\n",
    "0 Y 254 16.337\n",
    "0 Y 255 14.872\n",
    "```\n",
    "\n",
    "We need to filter out the header lines, which begin with a '#'.  We'll also make a simple class to represent each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isHeader(line):\n",
    "    return \"# \" in line\n",
    "\n",
    "class Record(object):\n",
    "    def __init__(self, trial, posn, sample, reading):\n",
    "        self.trial = trial\n",
    "        self.posn = posn\n",
    "        self.sample = sample\n",
    "        self.reading = reading\n",
    "\n",
    "    @classmethod\n",
    "    def parse(cls, line):\n",
    "        tokens = line.split()\n",
    "        trial = int(tokens[0])\n",
    "        posn = tokens[1]\n",
    "        sample = int(tokens[2])\n",
    "        reading = float(tokens[3])\n",
    "        return cls(trial, posn, sample, reading)\n",
    "\n",
    "data = sc.textFile(localpath(\"small_data/eeg/*\")) \\\n",
    "    .filter(lambda x: not isHeader(x)) \\\n",
    "    .map(Record.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an action for `RDD[Double]` called `stats` that will provide us with summary statistics about the values in the RDD. How can we get out summary stats of the `reading` column across the entire data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.map(lambda x: x.reading).stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we do the same but for only the position `FP1`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.filter(lambda x: x.posn == \"FP1\").map(lambda x: x.reading).stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure there are 256 samples per `posn` in this data set. How can we do this? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = data.map(lambda record: ((record.trial, record.posn), 1)) \\\n",
    "              .reduceByKey(lambda a, b: a + b)\n",
    "posns = samples.map(lambda c: (c[0][1], c[1])).distinct()\n",
    "\n",
    "print(posns.take(1))\n",
    "print(posns.count(), posns.filter(lambda x: x[1] == 256).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2018 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
