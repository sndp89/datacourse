{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.dpi'] = 100\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start Spark with BigDL support\n",
    "from pyspark import SparkContext\n",
    "import bigdl\n",
    "import bigdl.util.common\n",
    "sc = SparkContext.getOrCreate(conf=bigdl.util.common.create_spark_conf().setMaster(\"local[3]\")\n",
    "                              .set(\"spark.driver.memory\",\"1g\"))\n",
    "bigdl.util.common.init_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras, TensorFlow, Caffe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BigDL is still a very new player in the deep learning space.  TensorFlow and Caffe are much more established, and there are many systems that use them and pre-trained models you can download, such as the Inception model used in DeepDream.  In addition, Keras is slowly becoming a standardized interface for deep learning.\n",
    "\n",
    "As such, BigDL has support for all three.  We can load models in all three formats, and design our models for BigDL in Keras with a little work.  BigDL is not yet a proper Keras backend, but the ways models are designed in BigDL are very similar to how they are designed in Keras.  This has much to do with history, as BigDL's interface is actually modeled on that of Torch, another older deep learning interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with existing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is straightforward to save and load native BigDL models.  If we have a trained model, we can save it to a file (well, two files) with the `saveModel` function.  Let's do this with our simple logistic regressor from the Intro notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from bigdl.util.common import Sample\n",
    "\n",
    "centers = np.array([[0, 0]] * 150 + [[1, 0.5]] * 150 + [[0,1]]*150)\n",
    "np.random.seed(41)\n",
    "data = np.random.normal(0, 0.2, (450, 2)) + centers\n",
    "labels = np.array([[1]] * 150 + [[2]] * 150 + [[3]]*150)\n",
    "\n",
    "#Real data, of course, will be coming from elsewhere and will most likely already be an RDD at this point\n",
    "data_with_labels = zip(data, labels)\n",
    "samples = sc.parallelize(data_with_labels).map(lambda x: Sample.from_ndarray(x[0],x[1]))\n",
    "\n",
    "plt.scatter(data[:,0], data[:,1], c=labels.reshape(-1), cmap=plt.cm.brg)\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.nn import layer\n",
    "from bigdl.nn import criterion \n",
    "from bigdl.optim import optimizer\n",
    "\n",
    "lin = layer.Linear(2,3)()\n",
    "soft = layer.SoftMax()(lin)\n",
    "model = layer.Model([lin],[soft])\n",
    "\n",
    "fitter = optimizer.Optimizer(model=model, training_rdd=samples, \n",
    "                             criterion=criterion.ClassNLLCriterion(logProbAsInput=False), \n",
    "                             optim_method=optimizer.SGD(0.05), end_trigger=optimizer.MaxEpoch(20), \n",
    "                             batch_size=60)\n",
    "\n",
    "trained_model = fitter.optimize()\n",
    "\n",
    "def get_accuracy(predicts, trues):\n",
    "    return sum([int(predicts[i] == trues[i]) for i in range(len(predicts))]) * 1.0 / len(trues)\n",
    "\n",
    "predictions = model.predict(samples).map(lambda x: x.argmax() + 1).collect()\n",
    "get_accuracy(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order: file for graph/model shape, file for weights, whether to\n",
    "# overwrite existing model.  These can be local files, HDFS, or S3\n",
    "model.saveModel('./models/logistic.graph', './models/logistic.weights', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format is an internal BigDL-specific binary format.  It is fairly compact, but very much not human readable.\n",
    "\n",
    "To load it back in, we merely need to call `loadModel` with the same files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = layer.Model.loadModel('./models/logistic.graph', './models/logistic.weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can confirm that it's the same model by making the same prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = loaded_model.predict(samples).map(lambda x: x.argmax() + 1).collect()\n",
    "get_accuracy(predictions, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BigDL is quite interoperable with TensorFlow.  We can save our models to TF format and we can read in a TF model (both trained and untrained).\n",
    "\n",
    "Note that we defined our model using the Functional interface above.  That was intentional, as only the Functional models can be imported and exported to TensorFlow.  TensorFlow also needs to have names for the placeholder variables used for input, and their shape.  We'll have to supply that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to TF, untrained model\n",
    "# Order: list of inputs in the form (name, shape), then the file to save\n",
    "# to.  .pb is TF's format\n",
    "model.save_tensorflow([('input',[10,2])], './models/logistic.pb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read in a trained model from TensorFlow.  If it's a checkpoint file (just like BigDL's checkpoint files, they save the state as a model is trained), we'll need to do an extra step.\n",
    "\n",
    "In TF, these models are traditionally named  `something.ckpt.*` with the extra extension being `.meta`, `.data` and `.index`.  BigDL comes with a tool to turn these in to the `.pb` and `.bin` files that it needs called `export_tf_checkpoint.py`. This is an external python script you need to run, and it takes as arguments.  These files contain the graph definition (`.pb`) and the weight values (`.bin`).\n",
    "\n",
    "Once you have these, it's straightforward-ish to load the model.  You'll need to know what the input and output placeholders/variables were in the original TF graph.  We'll just read in the one we just wrote out and leave off the `.bin` part, so we'll get an untrained model.  If we had the `.bin` file, we'd need to add a `bin_file=` flag to our call.  Since this was a BigDL model, the output layer got a rather strange name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outname = model.flattened_layers()[-1].name()\n",
    "print(outname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = layer.Model.load_tensorflow('./models/logistic.pb', ['input'], [outname])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing a trained model: Caffe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Caffe, things are stored in external files by default, so we don't need special saving strategies.  If we have a text file describing an network architecture, called a `prototxt` and the trained model file, importing is another simple call:\n",
    "\n",
    "```python\n",
    "model = Model.load_caffe_model(prototxt_filename, model_filename)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing a trained model: Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras models are stored in JSON files, with trained weights in the `HDF5` binary format.  It is a front end rather than a full learning system, so it uses several different programs for its back end, notably TensorFlow and Torch.  While the `HDF5` files should be independent of choice of backend, BigDL is only tested with files from the TensorFlow backend.\n",
    "\n",
    "To load an untrained model, simply call:\n",
    "\n",
    "```python\n",
    "keras_model = layer.Model.load_keras(json_path=json_file)\n",
    "```\n",
    "\n",
    "With trained weights, you just need to also specify the `HDF5` file:\n",
    "\n",
    "```python\n",
    "keras_model = layer.Model.load_keras(json_path=json_file,\n",
    "                                     hdf5_path=hdf5_file,\n",
    "                                     by_name=False)\n",
    "```\n",
    "\n",
    "The `by_name` flag is optional, with a default of False.  It tells BigDL whether to ignore the names of the layers and fill in the model from the `HDF5` just following the architecture (False) or to only read in those layers that have the same name in both files, with layers in the JSON that are not in the other file given default weight values (True)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a model in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since BigDL is not a supported Keras backend yet, we cannot _train_ directly using Keras.  We can, however, make our models in Keras then read them in to BigDL.  Designing a model in Keras is much like designing models in BigDL, with both Sequential and Functional interfaces.  All Keras models have a `to_json()` function, which outputs the model structure as a JSON string.  If you write this to a file, you can then use the load call above.\n",
    "\n",
    "Why bother with this extra complication?  There are a lot of models out there already in Keras form, that you can download and manipulate or use as a jumping-off point.  And most deep learning utilities support Keras to some extent, so saving to the Keras JSON format means having your model in a format that anyone can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2018 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
