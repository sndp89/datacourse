{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.request import urlopen\n",
    "\n",
    "from pylib.draw_graph import draw_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding /opt/conda/envs/data3/lib/python3.6/site-packages/bigdl/share/lib/bigdl-0.6.0-jar-with-dependencies.jar to BIGDL_JARS\n",
      "Prepending /opt/conda/envs/data3/lib/python3.6/site-packages/bigdl/share/conf/spark-bigdl.conf to sys.path\n"
     ]
    }
   ],
   "source": [
    "#Start Spark with BigDL support\n",
    "from pyspark import SparkContext\n",
    "import bigdl\n",
    "import bigdl.util.common\n",
    "sc = SparkContext.getOrCreate(conf=bigdl.util.common.create_spark_conf().setMaster(\"local[2]\")\n",
    "                              .set(\"spark.driver.memory\",\"4g\"))\n",
    "bigdl.util.common.init_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- requirement: pylib/__init__.py -->\n",
    "<!-- requirement: pylib/draw_nn.py -->\n",
    "<!-- requirement: pylib/draw_graph.py -->\n",
    "<!-- requirement: small_data/strata_abstracts.txt -->\n",
    "\n",
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, we have been dealing only with **feed-forward** networks.  These take in some input features and feed those through the network to produce the output.  For many problems, this is enough.  If you want to classify images of cats, all you care about are the features from one given image.  You don't care about what the previous (or next) picture was.\n",
    "\n",
    "However, much of the world's data is time-dependent.  Some of this is very obvious: If you're trying to predict future stock prices, knowing the past prices is probably going to be useful.  Other cases might not be immediately clear.  A prime example is language processing.  Order is important; there's a big difference between \"dog bites man\" and \"man bites dog\".  Similarly, speech recognition, optical character recognition, and text summarization algorithms all benefit from knowing something about the previous inputs.\n",
    "\n",
    "We encountered the same sort of issue when we wanted to classify images.  Knowing what's nearby in space is very important, so we designed a network architecture to reflect those priorities.  In that case, we developed convolutional nets that combined features nearby in space.  Now, we need to design a network that gives us nearby-in-time features.\n",
    "\n",
    "Such networks are known as **recurrent** neural networks (RNN).  To illustrate how they work, we'll adopt a simple sketch notation.  Let this represent a feed-forward of the kind we've already dealt with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.36.0 (20140111.2315)\n",
       " -->\n",
       "<!-- Title: g Pages: 1 -->\n",
       "<svg width=\"260pt\" height=\"44pt\"\n",
       " viewBox=\"0.00 0.00 260.00 44.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 40)\">\n",
       "<title>g</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-40 256,-40 256,4 -4,4\"/>\n",
       "<!-- a -->\n",
       "<g id=\"node1\" class=\"node\"><title>a</title>\n",
       "<polygon fill=\"gray\" stroke=\"black\" points=\"36,-36 0,-36 0,-0 36,-0 36,-36\"/>\n",
       "</g>\n",
       "<!-- b -->\n",
       "<g id=\"node2\" class=\"node\"><title>b</title>\n",
       "<ellipse fill=\"#91bfdb\" stroke=\"black\" cx=\"90\" cy=\"-18\" rx=\"18\" ry=\"18\"/>\n",
       "</g>\n",
       "<!-- a&#45;&gt;b -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>a&#45;&gt;b</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M36.3034,-18C44.0173,-18 53.2875,-18 61.8876,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.8957,-21.5001 71.8957,-18 61.8956,-14.5001 61.8957,-21.5001\"/>\n",
       "</g>\n",
       "<!-- b1 -->\n",
       "<g id=\"node3\" class=\"node\"><title>b1</title>\n",
       "<ellipse fill=\"#91bfdb\" stroke=\"black\" cx=\"162\" cy=\"-18\" rx=\"18\" ry=\"18\"/>\n",
       "</g>\n",
       "<!-- b&#45;&gt;b1 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>b&#45;&gt;b1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M108.303,-18C116.017,-18 125.288,-18 133.888,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"133.896,-21.5001 143.896,-18 133.896,-14.5001 133.896,-21.5001\"/>\n",
       "</g>\n",
       "<!-- c -->\n",
       "<g id=\"node4\" class=\"node\"><title>c</title>\n",
       "<ellipse fill=\"#fc8d59\" stroke=\"black\" cx=\"234\" cy=\"-18\" rx=\"18\" ry=\"18\"/>\n",
       "</g>\n",
       "<!-- b1&#45;&gt;c -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>b1&#45;&gt;c</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M180.303,-18C188.017,-18 197.288,-18 205.888,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"205.896,-21.5001 215.896,-18 205.896,-14.5001 205.896,-21.5001\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7f739abb2438>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_graph(\"feed-forward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two blue circles represent hidden layers.  In the feed-forward architecture, they just feed activations further through the network.\n",
    "\n",
    "In contrast, we represent a recurrent network like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.36.0 (20140111.2315)\n",
       " -->\n",
       "<!-- Title: g Pages: 1 -->\n",
       "<svg width=\"188pt\" height=\"62pt\"\n",
       " viewBox=\"0.00 0.00 188.00 62.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 58)\">\n",
       "<title>g</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-58 184,-58 184,4 -4,4\"/>\n",
       "<!-- a -->\n",
       "<g id=\"node1\" class=\"node\"><title>a</title>\n",
       "<polygon fill=\"gray\" stroke=\"black\" points=\"36,-36 0,-36 0,-0 36,-0 36,-36\"/>\n",
       "</g>\n",
       "<!-- b -->\n",
       "<g id=\"node2\" class=\"node\"><title>b</title>\n",
       "<ellipse fill=\"#91bfdb\" stroke=\"black\" cx=\"90\" cy=\"-18\" rx=\"18\" ry=\"18\"/>\n",
       "</g>\n",
       "<!-- a&#45;&gt;b -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>a&#45;&gt;b</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M36.3034,-18C44.0173,-18 53.2875,-18 61.8876,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.8957,-21.5001 71.8957,-18 61.8956,-14.5001 61.8957,-21.5001\"/>\n",
       "</g>\n",
       "<!-- b&#45;&gt;b -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>b&#45;&gt;b</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M76.5578,-41.9516C77.1081,-48.7595 81.5889,-54 90,-54 102.234,-54 106.153,-42.9126 101.756,-32.0417\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"80.0169,-42.4871 78.2435,-32.0417 73.116,-41.3132 80.0169,-42.4871\"/>\n",
       "</g>\n",
       "<!-- c -->\n",
       "<g id=\"node3\" class=\"node\"><title>c</title>\n",
       "<ellipse fill=\"#fc8d59\" stroke=\"black\" cx=\"162\" cy=\"-18\" rx=\"18\" ry=\"18\"/>\n",
       "</g>\n",
       "<!-- b&#45;&gt;c -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>b&#45;&gt;c</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M108.303,-18C116.017,-18 125.288,-18 133.888,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"133.896,-21.5001 143.896,-18 133.896,-14.5001 133.896,-21.5001\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7f739abb2cc0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_graph(\"recurrent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the neuron is feeding information back to itself, creating a loop.  This is unstable&mdash;what is the solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State and time dependence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is _state_.  We don't feed the current output value back in to the node, we associate the output with a time step, and only feed the previous time's value in.  This does require an initial value, and it means that the neuron is now stateful, as it has to maintain information.  With this change, we can also now vary the input to have a time state as well, and instead of feeding in a single value as in the diagram, we feed in a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the hidden layers get input activations not just from the layer below, but from their (or other layer's) output at a previous step.  These hidden activations incorporate the previous state of the network, thereby providing some memory of the previous inputs.  During training, the recurrent network will learn how to weight both the current features and the previous state in making decisions.\n",
    "\n",
    "An alternative approach would be to have inputs not just for the current time, but for *n* previous times.  As long as the connections are set up appropriately, this could give similar performance.  However, it would require inputs of a fixed length.  In contrast, RNNs can take inputs of arbitrary length, since all of the previous input, of whatever length, is represented in the hidden activations.\n",
    "\n",
    "It was proven in 2006 by Schäfer and Zimmerman that RNNs with sigmoid activations are **Turing-complete**.  That is, any program can be written by giving a RNN the appropriate weights.  This is only a theoretical result; there is no method to find those weights for a particular task.  However, it's easier to use an optimization algorithm to explore the space of weight matrices than the space of Python programs, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfolded representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naively, taking the derivative of this system to be able to do our SGD is going to be very difficult.  It's going to be time dependent as well, and we'll need to worry about how long the input sequence is.  Also, directed cyclic graphs aren't terribly friendly to distributed computation, which will make things complicated with Spark.\n",
    "\n",
    "The trick is to **unroll** or unfold the RNN through time.  That is, the recurrent edge is pointed not back to the same node, but to another copy, representing the next step in time.  And in that copy, this edge points to yet another copy, giving us a structure like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(\"unrolled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although somewhat unusual, this is a perfectly good feed-forward network, and the standard backpropagation algorithm can be used with it.  The only wrinkle is that the weight matrices in all of the copies must stay in sync.  Therefore, we update them by summing all the gradients from all copies, similar to the way we handled convolutional filters.  This process in known as \"backpropagation through time\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is implemented naïvely, it will immediately run into problems with vanishing and exploding gradients.  To see why, recall that we can write the prediction of a feed-forward network as a set of nested functions.\n",
    "\n",
    "$$ p = f_n\\left( f_{n-1}\\left( \\cdots f_2\\left( f_1\\big( x W_1 \\big) W_2 \\right) \\cdots W_{n-1} \\right) W_n \\right) $$\n",
    "\n",
    "(We've dropped the bias terms for simplicity.)  To update $W_1$, for example, we need to calculate\n",
    "\n",
    "$$ \\frac{\\partial p}{\\partial W_1} = f_n' \\big(\\cdots\\big) W_n \\cdot f_{n-1}'\\big(\\cdots\\big) W_{n-1} \\cdot\\cdots\\cdot f_2'\\big(\\cdots\\big) W_2 \\cdot f_1'\\big(x W_1\\big) \\cdot x$$\n",
    "\n",
    "If each of these terms are greater than one, the whole gradient gradient will become large.  This will require a small learning-rate to avoid the optimizer diverging.  If each of the the terms are less than one, the gradient will vanish, and the weight won't change appreciably.\n",
    "\n",
    "This is a general problem in deep networks, but with a generic network we can at least hope that we'll have terms both less than and greater than one, so that the whole gradient remains finite.  In an RNN, though, these nested functions represent previous copies of the network, so all of the weights are the same!  Unless they happen to be exactly one, we're guaranteed to get an explosion or a vanishing!\n",
    "\n",
    "This issue has been dubbed the \"fundamental problem of deep learning\".  Several solutions have been proposed, but the current *de facto* standard is **long-short term memory**  (LSTM).  LSTM replaces the simple neurons with LSTM cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(\"LSTM_full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM gets around the problem by breaking the direct coupling between the previous input, the state, and the output.  The diagram shows a fairly typical LSTM cell, which replaces the neuron from simpler networks.  In our unrolled representation, we feed two pieces of information from one time step to the next: the output of the cell, and an internal state.  There are a few more moving parts to understand now, but they can be summarized as update state and generate output:\n",
    "\n",
    "$$C_{i} = g_{forget}\\otimes C_{i-1} + g_{input} \\otimes \\tanh(W_x\\cdot x_{i} + W_h\\cdot h_{i-1} + b)$$\n",
    "\n",
    "$$h_{i} = g_{output} \\otimes \\tanh(C_i)$$\n",
    "\n",
    "where $C_t$ is the internal state at time $t$, $h_t$ is the output of the cell for time $t$.  The operations with the circles around them are point-wise: $g$ and $C$ and $h$ all have the same shape, and you are multiplying individual elements.  What exactly that shape _is_ is a decision you get to make.  Often a scalar is chosen and you just make more cells, but vectors of various sizes are common as well.\n",
    "\n",
    "The various gates $g_m$ use a sigmoid function to produce values between $0$ and $1$. They act to control how much of the various components are used in each update step.  They are calculated from the input and previous output, so that decisions about how to weight new input against the stored state are made dynamically.  They are given by\n",
    "\n",
    "$$g_m(x_i,h_{i-1}) = \\mathrm{sigmoid}(W_x\\cdot x_i + W_h\\cdot h_{i-1} + b)$$\n",
    "\n",
    "where each gate has its own $W_x$ and $W_h$ parameters, and the sigmoid is again applied point-wise.\n",
    "\n",
    "There are several sophistications commonly added to the basic LSTM cell.  For example, peephole connections connect the internal state the the gates inside of the cell. There is some evidence that these improve the performance tasks requiring precise intervals.  There are also changes made to make them more compatible with convolutional outputs for processing video.\n",
    "\n",
    "These are both implemented in BigDL, but as separate `layer` types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent networks can be used for a number of different applications.  The examples we've draw so far illustrate **sequence labeling**.  For each input, we calculate all the way through to the output.  This could be used, for example, to classify words in a sentence by their part of speech.  We need a label for each word, and the recurrent nature lets us use the past words to differentiate between, for example, the word \"rows\" in \"Jack rows the boat\" and \"Jack walked between rows of wheat.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(\"unrolled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we care only about a single label for the whole sequence.  This is known as **sequence classification**.  We read the output associated only with the final input.  The recurrent nature ensures that this output has information from all of the inputs.  This could be used to classify the sentiment of a review text, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(\"classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The situation is essentially reversed for **sequence generation**.  In this case, we take only a single input and wish to generate a whole sequence of output.  We can do this by feeding the output from one step (suitably processed, perhaps) as input to the next time step.  This can be used to generate text that resembles some corpus, as we demonstrate below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(\"generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ideas of classification and generation can be combined in **sequence translation**.  Here, an input sequence is fed into a network to establish a certain internal state.  This state is then used as the start of a decode sequence.  This can be done with two related RNNs or with a single one.  In the latter case a special token is needed to indicate to the network when to start outputting the translated version, as illustrated below.  Google used such networks for its [recent improvement](https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html?_r=0) of its translation service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(\"translation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Name origins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's generally possible for a human to look at a last name and have a good idea of what country it originated from.  This should be more than just experience - different languages have different sound patterns (and different spelling, both natively and when they're transliterated in to English).  We should be able to build a learner to take advantage of this.\n",
    "\n",
    "This will be a case of sequence classification - each name will be treated as a sequence of characters, and we're seeking to classify them with what language they came from.  To this end, we have a list of last names sorted by language of origin, taken from the `PyTorch` website.  Each language is a separate file, consisting of one last name per line.\n",
    "\n",
    "Unfortunately, they're very different in number of names, so we will need to either restrict ourselves in which languages we choose, do some unbalanced class adjustments, or both.  But first, let's read in the data. It's quite small, so we'll do some of our pre-processing in Python for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will create data/names/<lang>.txt files\n",
    "!wget -nc https://download.pytorch.org/tutorial/data.zip\n",
    "!unzip -o data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#We'll lowercase and de-duplicate while we read in\n",
    "def get_names(filename):\n",
    "    with open(os.path.join(datapath, filename), encoding='utf-8') as f:\n",
    "        names = [name.strip().lower() for name in f.readlines()]\n",
    "    deduped = list(set(names))\n",
    "    return deduped\n",
    "\n",
    "datapath = 'data/names'\n",
    "langfiles = [f for f in os.listdir(datapath) if os.path.isfile(os.path.join(datapath, f))]\n",
    "\n",
    "names_by_lang = []\n",
    "for filename in langfiles:\n",
    "    lang = filename.split(\".\")[0]\n",
    "    names_by_lang.append((lang, get_names(filename)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get how many names for each language\n",
    "[(pair[0], len(pair[1])) for pair in names_by_lang]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very broad range in number of names.  We're going to take a hybrid approach by filtering out languages with fewer than 500 names, then artificially inflating the smaller categories until they have the same number as the largest category (Russian) by oversampling.  We could equally well choose a number in between, say 2000 or 4000, and normalize them all by either removing or repeating names as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out those with less than 500\n",
    "enough_names = [pair for pair in names_by_lang if len(pair[1]) > 500]\n",
    "print([pair[0] for pair in enough_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That leaves us with six languages, we can work with that.  Now let's make correctly sized samples, but remember to split off 10% for testing later.  It's *very* important we do the split first, before we do any sampling. Let's make two functions to allow us to do the split and do the up or down sampling as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_sample(namelist, n_samples):\n",
    "    #If len(namelist) > n_samples, randomly sample from list (without replacement)\n",
    "    #If len(namelist) < n_samples, put the whole list in, then sample _with_ replacement\n",
    "    if len(namelist) >= n_samples:\n",
    "        return random.sample(namelist, n_samples)\n",
    "    else:\n",
    "        #We'll shuffle the full list before putting it in, just to be safe\n",
    "        oversamples = n_samples - len(namelist)\n",
    "        return random.sample(namelist, k=len(namelist)) + random.choices(namelist, k=oversamples)\n",
    "    \n",
    "def split_test_train(namelist, test_split):\n",
    "    #test_split is between 0 and 1, representing fraction to reserve in test\n",
    "    shuffled = random.sample(namelist, k=len(namelist))\n",
    "    test_size = int(len(namelist)*test_split)\n",
    "    return [shuffled[:test_size], shuffled[test_size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = []\n",
    "trains = []\n",
    "test_fraction = 0.1 \n",
    "for langset in enough_names:\n",
    "    test, train = split_test_train(langset[1], test_fraction)\n",
    "    tests.append((langset[0], test))\n",
    "    trains.append((langset[0], train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's sample and put everything together to get our training sample.  Our Russian data set is the largest, so we'll use its size to guide our sample size.  Additionally, we'll convert everything to (name, language) pairs to have labeled data.  We'll also lowercase the names while we're at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = dict([(pair[0], len(pair[1])) for pair in trains])['Russian']\n",
    "\n",
    "training = []\n",
    "for langset in trains:\n",
    "    lang = langset[0]\n",
    "    names = langset[1]\n",
    "    training.extend([(name.lower(), lang) for name in get_sample(names, n_samples)])\n",
    "    \n",
    "#Here we'll do just the opposite - we'll take test samples of the size of the smallest one\n",
    "#this will keep the test set balanced, but isn't strictly necessary\n",
    "n_test = dict([(pair[0], len(pair[1])) for pair in tests])['Czech']\n",
    "testing = []\n",
    "for langset in tests:\n",
    "    lang = langset[0]\n",
    "    names = langset[1]\n",
    "    testing.extend([(name.lower(), lang) for name in get_sample(names, n_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training))\n",
    "training[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a bit more data massaging to go still - we need to convert the language labels to numbers (starting with one, not zero, for BigDL), and we need to convert the names in to one hot encoded characters.  We'll do this by hand, since there are some unicode characters such as ó and ñ (and, oddly, a 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#We'll do this over all of the data rather than just the training\n",
    "chars = set()\n",
    "for lang in enough_names:\n",
    "    for name in lang[1]:\n",
    "        chars.update(name.lower())\n",
    "    \n",
    "mapping = list(enumerate(chars))\n",
    "n_char = len(mapping)\n",
    "to_char = dict(mapping)\n",
    "from_char = dict([(x[1],x[0]) for x in mapping])\n",
    "\n",
    "# Custom one-hot encoder, since the BigDL one doesn't have a python interface\n",
    "def one_hot(n, length):\n",
    "    zs = np.zeros(length)\n",
    "    zs[n] = 1\n",
    "    return zs\n",
    "\n",
    "def one_hot_string(astring):\n",
    "    return np.array([one_hot(from_char[c], n_char) for c in astring])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(one_hot_string('alpha').shape)\n",
    "one_hot_string('alpha')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we had 44 distinct characters there, so we're getting NumPy arrays of shape (number of characters) by 44.  Now we just need to encode our languages and we should have the pieces we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = [x[0] for x in enough_names]\n",
    "mapping = list(enumerate(langs))\n",
    "n_lang = len(mapping)\n",
    "to_lang = dict(mapping)\n",
    "from_lang = dict([(x[1],x[0]) for x in mapping])\n",
    "\n",
    "def encode_lang(lang):\n",
    "    return from_lang[lang] + 1\n",
    "\n",
    "def decode_lang(code):\n",
    "    return to_lang[code - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encode_lang('English'))\n",
    "print(decode_lang(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give the data a good shuffle before we start, just to be safe.  Also, we note that we'll sort the data by length in time (in our case number of characters) due to a bug in BigDL that sets an internal maximum size in time based on the first element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.util.common import Sample\n",
    "random.shuffle(training)\n",
    "samples = sc.parallelize(training).sortBy(lambda x: len(x[0]), ascending=False)\\\n",
    "            .map(lambda x: (one_hot_string(x[0]), encode_lang(x[1])))\\\n",
    "            .map(lambda x: Sample.from_ndarray(x[0], x[1]))\n",
    "samples.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the final data is going to have a three dimensional shape (minibatch size $\\times$ time size $\\times$ one-hot size), so each individual Sample will be 2D, of size (time size $\\times$ one-hot size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layers of recurrent cells are typically not used alone.  At the very least, a full-connected feed-forward layer is used following the recurrent layer.  This translates the individual memories into the expected output.  More complicated architectures are common, with several recurrent and several feed-forward layers combined.  Convolutional layers may also be used when the problem calls for them, for example in processing video.\n",
    "\n",
    "When these are unrolled, only the recurrent layers are connected between time steps.  Each time step otherwise acts as a copy of the overall network, with all time steps having the same parameters.  In BigDL, we need to indicate which layers are getting this behavior, which we do with the `Recurrent` layer. This acts much as the `Sequential` holder - we will add things to it to build our recurrent network part, then add it to our model.\n",
    "\n",
    "This can be difficult to follow, so we're going to very carefully build up one layer at a time. We'll make a small test data set and track it through here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = np.array([one_hot_string('alpha'), one_hot_string('gamma'), one_hot_string('theta')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.nn import layer\n",
    "\n",
    "model = layer.Sequential()\n",
    "recurrent = layer.Recurrent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of options for cells we can put in to this container.  For this demonstration, we're going to use fairly basic LSTM cells, `LSTM`.  The elaborations have names like `LSTMPeephole`, see the BigDL documentation for a [list](https://bigdl-project.github.io/master/#APIGuide/Layers/Recurrent-Layers/).\n",
    "\n",
    "For our LSTM cell, we'll need to specify the size of the input (i.e. the length of $x$, so the number of states in our one-hot encode) and the size of the state $C$.  Since all of the sums and multiplications in LSTM are done point-wise, there is no meaningful distinction between an array of LSTM cells and a single cell with a larger state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 64\n",
    "# This can contain *exactly one* recurrent cell.\n",
    "# If you want more, make a second Recurrent container.\n",
    "recurrent.add(layer.LSTM(n_char, hidden_size))\n",
    "model.add(recurrent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(small_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A second layer of LSTM\n",
    "recurrent2 = layer.Recurrent()\n",
    "recurrent2.add(layer.LSTM(hidden_size, hidden_size))\n",
    "model.add(recurrent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(small_data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That takes care of the recurrent parts&mdash;we'll get out (minibatch size $\\times$ time $\\times$ hidden size) at the end.  We'll want to feed this in to an otherwise normal linear (dense) layer.  However, BigDL will *not* handle this as-is.\n",
    "\n",
    "The reason?  We need to tell it how we want the linear layer to interact with the recurrent - we can either use some sort of selector to pick off part of the time output, or we can tell BigDL that we want the linear function to be run on each time step.\n",
    "\n",
    "The former would be the behavior we would be after if we were doing sequence labeling or generation, so we'd have an output for each time step.  In that case, we'd follow our recurrent layer with \"time distributed\" layers such as\n",
    "\n",
    "```python\n",
    "model.add(layer.TimeDistributed(layer.Linear(hidden_size,output_dim)))\n",
    "model.add(layer.TimeDistributed(layer.SoftMax()))\n",
    "```\n",
    "\n",
    "If you do this, you get an output of size (minibatch size $\\times$ time size $\\times$ output dimension).  We have an example of this later in the notebook that we will set up but not run.\n",
    "\n",
    "But we're doing sequence classification, so we want a single output per sequence.  To do this, we just need to pick off the last one (in the time dimension) for each sample, so we wind up with (minibatch size) $\\times$ (number of languages).  We do this with a `Select` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the last (-1) element of the time dimension - \n",
    "#This is dimension \"2\" since BigDL uses one-based counting for Scala reasons\n",
    "model.add(layer.Select(2,-1))\n",
    "model.add(layer.Linear(hidden_size, n_lang))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(small_data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is built.  It's worth stopping to note again that the middle value in those shapes (the time length) is dynamic: the model will adjust to any length by unrolling.  But as noted before, the system performs best if the first element in each minibatch (and in the overall training) is the largest.  If you do not do this, you'll likely see `java.lang.ArrayIndexOutOfBoundsException` errors.\n",
    "\n",
    "Since our output is a single, seven element array for each input row, we don't need to do anything special.  We can use the standard `CrossEntropyCriterion`, all the time related things have been dealt with internally in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.nn import criterion \n",
    "from bigdl.optim import optimizer\n",
    "\n",
    "#We'll use RMSProp, claim is it works better on RNNs\n",
    "optim = optimizer.RMSprop(learningrate=0.03, learningrate_decay=0.9)\n",
    "evaluate = criterion.CrossEntropyCriterion()\n",
    "\n",
    "fitter = optimizer.Optimizer(model=model, training_rdd=samples, \n",
    "                             criterion=evaluate, \n",
    "                             optim_method=optim, \n",
    "                             end_trigger=optimizer.MaxIteration(10), \n",
    "                             batch_size=256)\n",
    "\n",
    "now_string = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "trainSummary = optimizer.TrainSummary(\"./logs\", \"rnn_{}\".format(now_string))\n",
    "trainSummary.set_summary_trigger(\"Loss\", optimizer.SeveralIteration(100))\n",
    "fitter.set_train_summary(trainSummary)\n",
    "\n",
    "#Add checkpointing, this will be about once every 4 epochs\n",
    "fitter.set_checkpoint(optimizer.EveryEpoch(), './logs/rnn_names_checkpoint_{}'.format(now_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to train.  We'll get a baseline first with almost no training, then check on it every so often to see how it's doing.  We'll measure the crude accuracy on our test and training sets so we can see it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(testing)\n",
    "test_samples = sc.parallelize(testing).sortBy(lambda x: len(x[0]), ascending=False)\\\n",
    "                 .map(lambda x: (one_hot_string(x[0]), encode_lang(x[1])))\\\n",
    "                 .map(lambda x: Sample.from_ndarray(x[0], x[1]))\n",
    "\n",
    "def print_accuracy(test_model):\n",
    "    training_prediction = test_model.predict(samples).map(lambda x: decode_lang(x.argmax() + 1)).collect()\n",
    "    training_label = samples.map(lambda x: decode_lang(x.labels[0].storage.tolist())).collect()\n",
    "    training_accuracy = sum([x[0] == x[1] for x in zip(training_prediction, training_label)]) / len(training_label)\n",
    "    \n",
    "    testing_prediction = test_model.predict(test_samples).map(lambda x: decode_lang(x.argmax() + 1)).collect()\n",
    "    testing_label = test_samples.map(lambda x: decode_lang(x.labels[0].storage.tolist())).collect()\n",
    "    testing_accuracy = sum([x[0] == x[1] for x in zip(testing_prediction, testing_label)]) / len(testing_label)\n",
    "    print(\"Current accuracy: training: {:.3}, testing: {:.3}\".format(training_accuracy, testing_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter.optimize()\n",
    "print_accuracy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "last_time = start_time\n",
    "\n",
    "#Increase to at least 21 or more to start real training\n",
    "#Note that range(1,11) will give you epoch 1, 2, ..., 9, 10 (not 11)\n",
    "epochs = range(1,2)\n",
    "\n",
    "for epoch in epochs:\n",
    "    fitter.set_end_when(optimizer.MaxEpoch(epoch))\n",
    "    fitter.optimize()\n",
    "    print(\"Training epoch {} took {:.2f} seconds\".format(epoch, time.time() - last_time))\n",
    "    if(epoch % 10 == 0):\n",
    "        print_accuracy(model)\n",
    "    last_time = time.time()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(\"Training finished after {:.2f} minutes\".format(total_time / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can take a look at the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order: step, value, timestamp\n",
    "import matplotlib.pyplot as plt\n",
    "losses = np.array(trainSummary.read_scalar('Loss'))\n",
    "plt.plot(losses[:,0],losses[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the accuracy across all test samples, but we need to be cognizant that this is a very heavily biased set, with many more Russian and English names than the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tests = [(name.lower(), lang) for (lang, names) in tests for name in names]\n",
    "all_test_samples = sc.parallelize(all_tests).sortBy(lambda x: len(x[0]), ascending=False)\\\n",
    "                 .map(lambda x: (one_hot_string(x[0]), encode_lang(x[1])))\\\n",
    "                 .map(lambda x: Sample.from_ndarray(x[0], x[1]))\n",
    "        \n",
    "testing_prediction = model.predict(all_test_samples).map(lambda x: decode_lang(x.argmax() + 1)).collect()\n",
    "testing_label = all_test_samples.map(lambda x: decode_lang(x.labels[0].storage.tolist())).collect()\n",
    "testing_accuracy = sum([x[0] == x[1] for x in zip(testing_prediction, testing_label)]) / len(testing_label)\n",
    "print(testing_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another example, we will use sequence generation to create text.  We will train a neural network on a corpus of data consisting of abstracts of talks from past conferences.  The networks will be trained to predict the next letter that appears in the abstract, *i.e.* we'll be making our predictions at the letter level, so the RNN will need to learn English as part of the process.  Once trained, the network can be used to generate an abstract by starting it off with a seed, and then continuing the sequence with letters chosen according the the probabilities output by the network.\n",
    "\n",
    "We start by reading in the data we've already downloaded.  The abstracts are saved in a text file, one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = open('small_data/strata_abstracts.txt', 'r').read().lower()\n",
    "\n",
    "print(len(txt))\n",
    "print()\n",
    "print(txt[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll want to split this in to individual characters and one-hot encode them, so first we need to convert them to numbers.  We could just take their ASCII values, but this would give us a larger vocabulary than we need.  Instead we work out our own encoding, based on the characters we actually see.  We'll ignore capitalization to make our life easier.\n",
    "\n",
    "Also, we're doing this manipulation in Python rather than in Spark for convenience, as we want to include the newlines as explicit characters to avoid issues with some of the abstracts being very short. (One is \"Details to come...\".)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(set(txt))\n",
    "data = [chars.index(c) for c in txt]\n",
    "n_chars = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility function to go back and forth\n",
    "def from_char(char):\n",
    "    return chars.index(char)\n",
    "\n",
    "def to_char(num):\n",
    "    return chars[num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a mapping, we need to look at how we're going to arrange our data.  We want to predict the next character based on the current character and the internal state.  We also want to have it work over a reasonable number of time steps in the training.  Too many, encompassing entire abstracts, will take a very long time.  Too few, and we won't do a good job of learning the state.  We'll choose 100 characters as a compromise.  In addition, we don't want to just divide it in to sequential 100 character chunks, we'll want to have it look at randomly selected, potentially overlapping 100 character chunks, since the system has memory.\n",
    "\n",
    "For each of these characters, the target is simply the next character.  We'll make our samples reflect that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random chunk of length n, with target\n",
    "def get_chunk(alist, n):\n",
    "    start = np.random.randint(len(alist) - n - 1)\n",
    "    return [alist[start:start+n],alist[start+1:start+n+1]]\n",
    "\n",
    "get_chunk(list('once upon a midnight dreary, while I pondered, weak and weary'), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom one-hot encoder, since the BigDL one doesn't have a python interface\n",
    "def one_hot(n, length):\n",
    "    zs = np.zeros(length)\n",
    "    zs[n] = 1\n",
    "    return zs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot(4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_chunk(data,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have each set of data be randomly drawn from our text corpus.  We could do this here in the master and then send that out, but instead we'll have it act in parallel.  Note that this does involve sending out the entire corpus to each worker, but since it's fairly small (just a few megabytes) it's not a concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encoded data\n",
    "def one_hot_chunk(alist, chunk_size):\n",
    "    chunk = get_chunk(alist,chunk_size)\n",
    "    return [np.array([one_hot(x, 64) for x in chunk[0]]), np.array(chunk[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_chunk(data,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to create our data set.  Since this is just an example model, we're going to set the parameters such that it will run quickly and produce poor output.  If you want to adjust this for a significantly longer run, set the parameters in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase this to 50000 for real run.\n",
    "# It will take quite a while.\n",
    "training_iters = 10\n",
    "\n",
    "# Turn this up to 10k or more for a real run.\n",
    "n_samples = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.util.common import Sample\n",
    "\n",
    "# That +1 on the label is *mission critical*, as the CrossEntropyCriterion\n",
    "# throws a \"BoxedError\" message if you forget it.  This is a very generic\n",
    "# Future failure.\n",
    "samples = sc.parallelize(range(n_samples)).map(lambda x: one_hot_chunk(data,100))\\\n",
    "             .map(lambda x: Sample.from_ndarray(x[0], x[1]+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the final data is going to have a three dimensional shape (minibatch size $\\times$ time size $\\times$ one-hot size), so each individual Sample will be 2D, of size (time size $\\times$ one-hot size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build a model nearly identical to our name checking model from before.  We'll print out the shapes as we go, but leave off the commentary except for the last bit, which is slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = np.array([[one_hot(from_char(x),64) for x in 'small test!'], \n",
    "                       [one_hot(from_char(x),64) for x in 'small test?']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.nn import layer\n",
    "\n",
    "model = layer.Sequential()\n",
    "recurrent = layer.Recurrent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "# This can contain *exactly one* recurrent cell.\n",
    "# If you want more, make a second Recurrent container.\n",
    "recurrent.add(layer.LSTM(n_chars, hidden_size))\n",
    "model.add(recurrent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(small_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A second layer of LSTM\n",
    "recurrent2 = layer.Recurrent()\n",
    "recurrent2.add(layer.LSTM(hidden_size, hidden_size))\n",
    "model.add(recurrent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(small_data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're doing sequence generation, where we want each step to have a prediction of the next character.  In this case we'll need to distribute the linear layer in time, so there's a copy for each time step.  This is accomplished through a different recurrent-type container.  We'll also apply a softmax layer in the same manner to make some later computations easier.\n",
    "\n",
    "Technically we could have also added another LSTM layer with output size `n_chars`, but that is more complexity than we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layer.TimeDistributed(layer.Linear(hidden_size,n_chars)))\n",
    "model.add(layer.TimeDistributed(layer.SoftMax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(small_data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training proceeds as before, with one change: we aren't just evaluating one value for each sequence, we're evaluating the output for each time step.  To do this, we'll need to time distribute our evaluation criterion as well using `TimeDistributedCriterion`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.nn import criterion \n",
    "from bigdl.optim import optimizer\n",
    "\n",
    "#We'll use RMSProp, claim is it works better on RNNs\n",
    "optim = optimizer.RMSprop(learningrate=0.03, learningrate_decay=0.9)\n",
    "evaluate = criterion.TimeDistributedCriterion(criterion.ClassNLLCriterion(logProbAsInput=False), size_average=True)\n",
    "\n",
    "fitter = optimizer.Optimizer(model=model, training_rdd=samples, \n",
    "                             criterion=evaluate, \n",
    "                             optim_method=optim, \n",
    "                             end_trigger=optimizer.MaxIteration(10), \n",
    "                             batch_size=12)\n",
    "\n",
    "now_string = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "trainSummary = optimizer.TrainSummary(\"./logs\", \"rnn_{}\".format(now_string))\n",
    "trainSummary.set_summary_trigger(\"Loss\", optimizer.SeveralIteration(1000))\n",
    "fitter.set_train_summary(trainSummary)\n",
    "\n",
    "#Add checkpointing\n",
    "fitter.set_checkpoint(optimizer.SeveralIteration(2000), './logs/minst_cnn_checkpoint_{}'.format(now_string))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate new text, we should just need to feed it characters, one at a time, and ask for the next character.  To do this, we would need to rely on the internal state moving forward with each character.  Unfortunately, this behavior is not implemented in the PySpark version of BigDL as of version 0.5.0\n",
    "\n",
    "Instead, we're going to go through a much slower process: we'll take a seed string and ask for the prediction for the next letter.  We'll then add that predicted character to our seed to make a new one, and as for the prediction on the new string, and so forth.  We're setting this up *before* we train so that we can stop the training occasionally to see what kind of strings it is generating.  As time progresses, we expect to see things that look progressively more like words, then more like conference abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double bracket to get right shape for model.predict.\n",
    "# Remember, our output is 1 greater than our input.\n",
    "def list_to_input(num_list):\n",
    "    return np.array([[one_hot(x-1,64) for x in num_list]])\n",
    "\n",
    "def choice(in_prob):\n",
    "    # Gets in list of probabilities, we'll make sure they sum to 1.\n",
    "    # Returns an random index, with the probability of each index\n",
    "    # given by the probs that were input.\n",
    "    # So [0.4,0.1,0.5] would give 0 40% of the time, 1 10%, and 2 50%.\n",
    "    probs = np.array(in_prob) *1.0 / sum(in_prob)\n",
    "    cum_prob = np.cumsum(probs)\n",
    "    p = np.random.uniform(0,1)\n",
    "    #assured that last value is 1, so you'll always get at least one True\n",
    "    #argmax takes the first occurence of the max value (True > False here)\n",
    "    return (cum_prob > p).argmax()\n",
    "\n",
    "# We'll do a probabilistic prediction to keep things interesting\n",
    "def next_char(num_list):\n",
    "    probs = fitted_model.predict(list_to_input(num_list))[0,-1,:]\n",
    "    # Returns a random number index with weight probs\n",
    "    return choice(probs) + 1\n",
    "\n",
    "def output_string(seed, length):\n",
    "    num_list = [from_char(x)+1 for x in seed]\n",
    "    for i in range(length):\n",
    "        c = next_char(num_list)\n",
    "        num_list.append(c)\n",
    "    return ''.join([to_char(x-1) for x in num_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the implementation is a little complicated, the concept is as outlined above.  We've also added a small tweak that we don't just take `argmax` to get the prediction, we will pick a random character with the probabilities given by the softmax, so we don't get stuck in any loops.\n",
    "\n",
    "We'll print the string generated in this manner every so often as we go.  Because we're using such a slow generation method, this is a touch time consuming, so we don't do it as often as would be ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fitted_model = model\n",
    "print(output_string('an',30))\n",
    "print()\n",
    "\n",
    "# 10 steps, just to get it started.\n",
    "fitted_model = fitter.optimize()\n",
    "print(output_string('an',30))\n",
    "print()\n",
    "\n",
    "for steps in range(3000, training_iters, 3000):\n",
    "    fitter.set_end_when(optimizer.MaxIteration(steps))\n",
    "    fitted_model = fitter.optimize()\n",
    "    print(output_string('an',30))\n",
    "    \n",
    "fitter.set_end_when(optimizer.MaxIteration(training_iters))\n",
    "fitted_model = fitter.optimize()\n",
    "print(output_string('an',30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get decent results out of this, we need to train for a very long time.  Instead of letting this run for a few hours, we'll look at the training output from a model that was allowed to run longer.\n",
    "\n",
    "```\n",
    "anw n;<v!-(o\"$r\\':rcpu3a,%#g,8jzwq#99c:5>b`ge|#68f>on\"ke@.t)^@&|u+=0$9k5t4\\n9rm7*zj!)8$61 07>wikl*=q&-;*\n",
    "\n",
    "annguomfndmhnsunsosdsrgsndsnhghhndsccddoshusnonhsdhdsuhdmdnnosmnoshhggnohsrcdssunnnccronosngosgshsnomn\n",
    "\n",
    "creating: createMaxIteration\n",
    "and earh ohnenene  r.cuuiwsni rei ouez trn aae lerla.a-fnemtocn  bdsdhtepren osnt naneo eur  bika ktln\n",
    "creating: createMaxIteration\n",
    "an  tsrertnfocgso.fo nuanyy tya deeeaoll f  a sayast rsotiuap  mciaas en ealvf mhmoannibiueo hcoa.  e \n",
    "creating: createMaxIteration\n",
    "ani sisd renisldes hdhsct.o teiip gpn  ivee pa  edl cvnt aiiaee ,encre viqnenrdopo evmse inpotd tmdet'\n",
    "creating: createMaxIteration\n",
    "anits a5j ristlocseslisev liwifetshrittn  lhrs,in cffn, aruo bale \n",
    "dimr tent  mlt lr a'teito-,s iasg p\n",
    "creating: createMaxIteration\n",
    "ann thysrdl feng tkecit-ad (omtia-davht af onoles ta lns tlide tongeus es ootris ave on rhe heke erd,e\n",
    "creating: createMaxIteration\n",
    "angs. tiros toicyle bicte\n",
    "thiticeiet ir uncesotg curharw is thots ams te salar bw tusgs narelnengor an\n",
    "creating: createMaxIteration\n",
    "anude ta ta roeda\n",
    "nes detis ure ana sudhos.\n",
    "(asumes emte thr op waaf rnaaltie sorpre 0sa fapci demtav \n",
    "creating: createMaxIteration\n",
    "anfflto thos tor ermore mutertog tice cheun? vermunisg ecre,tcin ud avetaster, dedchetser iog lears ec\n",
    "creating: createMaxIteration\n",
    "an, sancr dhigent ard aversing an and heduer-fa dalcale) ta tawy des prith goginy, data inthemkfss wel\n",
    "creating: createMaxIteration\n",
    "an dovatalinlegace appress acl, thecs boumissliwit, beetinn of mans in colews, thes fon tom sectariny,\n",
    "creating: createMaxIteration\n",
    "anad the sumoninss sunberegts, sall yuplane to uchute mase thil tomes a poliond aaving icpatasy enfpri\n",
    "creating: createMaxIteration\n",
    "and meverss acan urpreverils, but is ethl bat woll uged opding ale flit with fim clandtwerss.\n",
    "te opton\n",
    "creating: createMaxIteration\n",
    "ancuns can orel the famens mrever and da-procister nblectenegation of for and thet expriefhe recille s\n",
    "creating: createMaxIteration\n",
    "and farg will diall that a cint loepthen op fpoclety bralybse abalysig toct -ucthimy and da\"dovec? mok\n",
    "creating: createMaxIteration\n",
    "ang embat amd trourion fradolat for. for heveryxor wh'l produty prehove acaloging sol-tionsive using t\n",
    "creating: createMaxIteration\n",
    "an. youk ecpaccing os sokt and 2pon the sos umordic fortuce.\n",
    "nexysterms. whiil dotiess roweld -ferule,\n",
    "creating: createMaxIteration\n",
    "and incemoros intiil rneventer svort and thit seenf dithod recen-ners we to the vire deimd frower of h\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from this output that we probably ran for longer than needed.  It seems to have saturated about half way through, most likely due to our very limited sample size.  The very slow training speed is coming from the fact we're *not* running on Intel CPUs on this deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2018 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
